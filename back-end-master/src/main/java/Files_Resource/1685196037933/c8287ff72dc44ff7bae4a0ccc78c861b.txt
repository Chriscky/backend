Overall	Text
1	Need to document that the Go direct runner exists on https://beam.apache.org/documentation/runners/direct/
1	https://camel.apache.org/.Apache Camel has a large ecosystem of connectors, many of which are unavailable in Beam currently.This issue tracks the effort to create wrappers around Camel to gain access to these connections in Beam.Initially, these wrappers are focused exclusively on correctness, and are designed to enable access to new resources. There is not an initial focus on performance.Experiment around to find potential techniques for wrapping Camel.Draft design document, and get approval from the Beam community.Implement baseline wrappers.Improve wrappers
1	Steps to reproduce:Open Beam ZetaSQL lexical structure in the browser/Scroll to the section Reserved Keywords/Reduce the width of the page to 483px/Take a look how content of 1st column overlaps content of the 2nd column.Actual result: content of the columns overlaps each other.Expected result: content of the columns doesn't overlap each other
-1	Link to notebook.There are 3 issues with the notebook to fix up.Remove the logs with pip install (not necessary and clutters up the notebook).Ignore lfs error in colab, it is already installed. See Git LFS install fails on Google Colab even after successful installation using package cloud installer script. git-lfs/git-lfs#3605.Colab is failing on this line
-1	finalize() has been deprecated in Java 9 and has defects and per #24180 it is suspected that its usages might be cause of resource leak related bugs. We should remove its usage whenever possible.
1	Implement the following features for the RedisIO connector in the Go SDK
-1	Jobs reading from the BigQuery Storage API can hit Quota limits and eventually fail with The worker lost contact with the service. (Dataflow runner).The reason for the failure can be diagnosed on the API Monitoring (GCP) for BigQuery Storage API calls, where a spike in 429s can be seen for ReadRows when the throughput of the job starts going downhill.On the GCP side we've tried to resolve it through Quota increases, but it would be helpful to de-risk jobs crashing.related to:Bump throttling counter on BigQueryRead retries due to RESOURCE_EXHAU… #15445 <- Shouldn't this PR have pervented this kind of issue?
1	FirestoreIO should have a similar functionality like DatastoreIO does with EntityToRow/RowToEntity as that is still a missing "link" between the traditional beam schema/row and the firestore-related types.
1	I find that beam is using jackson and fastjson at the same time, why not remove fastjson? I think jackson is enough to parse a json object
1	The only current way to directly implement relational joins between two PCollections using the library is through Java SDK's org.apache.beam.sdk.schemas.transforms.Join. As a Python pipeline developer I would like to be able to do the same, in a similar fashion as described in the usage examples of https://beam.apache.org/documentation/sdks/java-extensions/#join-library.add-labels python,'new feature'
-1	Right now, we publish snapshots with the tag latest even if we're publishing a snapshot of an old version.We should only add the latest tag to snapshots from master so that tooling doesn't accidentally pick up old versions
-1	I have a query with the coll parameter in apache_beam.io.WriteToMongoDB.I would like to like to set the collection name dynamically based on the record, instead of hard coding it. For ex. if the record is dict. and if it has something like record.tag = 'example', I would like to retreive that and set as coll.I got some similar references for BigQuery and tried coll = lambda record: record.tag, which doesn't work. It threw an error saying it needs a string.
1	Currently Spanner change streams are supported as a source in Java, there is no current support in Python.Would be great to see this added to the Python SDK.
-1	Hi,I recently encountered an issue when upgrading my apache beam SDKs. I was attempting to upgrade from 2.30.0 to 2.40.0, and there was a file descriptor leak where approximately every time a new pipeline was started, java would create a new set of file descriptors to the libraries on the classpath. This resulted in many duplicate file descriptors being open and eventually hitting the maximum number of file descriptors.I was able to track it down to when I bump from 2.35.0 to 2.36.0, and I am 99% certain that this commit is the source.The main difference in this code is that classGraph.disableNestedJarScanning().addClassLoader(classLoader).scan(1).getClasspathFiles(); creates a new Scanner object with performScan=true,whereas classGraph.disableNestedJarScanning().addClassLoader(classLoader).getClasspathFiles(); does the same with performScan=false.PerformScan=true runs an asynchronoous scan of the classpath, and PerformScan=false just generates a placeholder ScanResult with just the classpath. The scan itself is creating the fd leak.The file leak itself is coming from the classgraph scanner.Here's one example of an open file descriptor
-1	Our configuration is using kubernetes to kick off a google cloud dataflow using the apache beam sdk.Please let me know if there is any other information I can provide.Is anyone else hitting a similar issue?
1	Is there any way to control the amount of sharing on the to_parquet call? (I tried adding n_shards = N, but seems to have no effect)
-1	Pandas to_feather fails when used with a non-default index:ValueError: feather does not support serializing a non-default index for the index; you can .reset_index() to make the index into column(s).Unfortunately, our DataFrame partitions will only have a default index in rare cases. Typically each partition will have some hash-based index. So to_feather will usually fail with the above error at execution time.
-1	Currently, when BigQueryIO fails to write to BigQuery, we get back a PCollection<BigQueryInsertError>via getFailedInsertsWithErr (or a PCollection<BigQueryStorageApiInsertError> if using getFailedStorageApiInserts), which provides us the TableRow for each failure.However, it would also be very useful to have access to the original input data and not just the transformed TableRow. In our use case, we stream Avro data from Kafka to BigQuery, so the input for BigQueryIO.Write is a KafkaRecord<String, byte[]>, which we transform into a TableRow via withFormatFunction. What we'd like to be able to do is write each failed insert back to Kafka (into a DLQ topic) so that we can reprocess it later on, however the only way we can currently achieve this is to convert the TableRow back into a KafkaRecord, which runs the risk of losing/transforming the original data during the conversion process.One possible workaround we've explored is joining the input PCollection containing the Kafka data with the failed inserts via some shared ID, so that we can get back the original messages. The main issue with this is that errors can sometimes take many hours to be visible via getFailedStorageApiInserts (due to #23291), so we would need to buffer many millions of records to cover this time window, which isn't feasible in our case.
-1	The following code doenst seem to work.There are no error messages reported in the logs as well.There are no quota issuesThis code is used in DataFlow pipeline
-1	When writing data with BigQueryIO using BigQueryIO.Write.Method.STORAGE_API_AT_LEAST_ONCE or BigQueryIO.Write.Method.STORAGE_WRITE_API, it would be very useful to be able to specify one or more SchemaUpdateOption values via .withSchemaUpdateOptions.Currently this only works for BigQueryIO.Write.Method.FILE_LOADS, which means that for other insert methods new fields are either ignored (if ignoreUnknownValues is set), or fail to insert. In our use case, we are streaming Avro records from Kafka to BigQuery, with the schema constantly changing (mostly new fields). Having BigQueryIO manage this would be a huge benefit.
1	Trying to use typing.Iterable[T] in Beam code works fine, but collections.abc.Iterable[T] does not.I imagine that the special casing of the typing module should be extended to collections.abc as well.
-1	Currently, we omitted arguments when converting from/to SqlTypeName.VARBINARY, SqlTypeName.BINARY, SqlTypeName.VARCHAR, SqlTypeName.CHAR types in Calcite.Utils:To create sql types, we should use the overload method RelDataTypeFactory.createSqlType(SqlTypeName, int) which accepts a second int parameter to construct these types;To create beam logical types, we should obtain the precision parameter of the Calcite type.
-1	Automate removal of 'awaiting triage' label for Playground changes. There is no need for triage so the Label can be removed by default
1	Configure master branch auto-pull from upstream apache/beam repo
1	Amend .\beam\playground\frontend\playground_components\lib\src\cache\example_cache.dart to remove special case for the MinimalWordCount example
-1	Problem Encountered:For a BigQueryIO.Write configured like in [1], the if target table doesn’t exist, then pipeline throws 404 Table Not Found exception and continuously retries the work item [2].Whereas for insert errors (broken json or schema error), it is able to catch the error (via getFailedInsertsWithErr).It was most recently reproduced on Apache Beam SDK for Java 2.39.0.What you expected to happen:Table not found errors should be caught by getFailedInsertsWithErr so that those records can be handled separately (like writing to dead letter queue or to GCS etc.)[1]WriteResult writeResult = results.get(SUCCESS_TAG).apply("WriteSuccessfulRecordsToBQ", BigQueryIO.writeTableRows() .withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS) .withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors()) //Retry all failures except for known persistent errors. .withWriteDisposition(WRITE_APPEND) .withCreateDisposition(CREATE_NEVER) .withExtendedErrorInfo() //- getFailedInsertsWithErr .ignoreUnknownValues() .skipInvalidRows() .withoutValidation() .to((row) -> { String tableName = Objects.requireNonNull(row.getValue()).get("event_type").toString(); return new TableDestination(String.format("%s:%s.%s", BQ_PROJECT, BQ_DATASET, tableName), "Some destination"); })[2]Error message from worker: java.lang.RuntimeException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found POST https://bigquery.googleapis.com/bigquery/v2/projects/dfdfdfdfdfd/datasets/sdfsdfdsfsfs/tables/dddddddd/insertAll?prettyPrint=false { "code" : 404, "errors" : [ { "domain" : "global", "message" : "Not found: Table dfdfdfdfdfd:sdfsdfdsfsfs.dddddddd", "reason" : "notFound" } ], "message" : "Not found: Table dfdfdfdfdfd:sdfsdfdsfsfs.dddddddd", "status" : "NOT_FOUND" } org.apache.beam.sdk.io.gcp.bigquery.BigQueryServicesImpl$DatasetServiceImpl.insertAll(BigQueryServicesImpl.java:1108) org.apache.beam.sdk.io.gcp.bigquery.BigQueryServicesImpl$DatasetServiceImpl.insertAll(BigQueryServicesImpl.java:1161)
-1	FhirIO connector does not exists for python SDK, so it could be an initial step to make it available for Python users via cross-language transform. The transforms would need a wrapper as the output types are not supported by cross-language.
1	Currently in ExampleBase only path is used to compare examples.This makes tests to compare example objects more complex than they should be.Need to:Refactor current comparison cases to only compare path if this is what they really should do.Include all (or most of the) fields in comparison.
1	Google finds the following page at the top, which has not been updated since 2021 and is not up to date.https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beamThe correct page is:https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam
-1	We should have a test that runs with the dataflow legacy_container to ensure it isn't broken. This container:
1	I originally asked this question on StackOverflow, but it does appear that there's a gap between python/java functionality and Go. According to this documentation, if emitting data into a PCollection from finish_bundle(), you must use a WindowedValue.This concept doesn't exist for the Go SDK and any data that's emitted from FinishBundle() is emitted into the SingleGlobalWindow. I see a TODO comment in the StartBundle() method that refers to a resolved Jira Issue BEAM-3303, but I don't see any existing Github Issues that address this gap.The below isn't a functioning example, but is essentially what I would like to be able to do in a streaming pipeline.
1	Right now, we're custom installing Beam latest in our Dataframe notebook - once 2.43 is released we should update to just install 2.43 directly.
1	See #23909 (comment).We use three different formats to represent "type hint" in Python documentation and docstrings. We should pick one and standardize on it across all documentation on the Beam website and in docstrings.
1	BigQueryIO currently documents triggering frequency as:This is only applicable when the write method is set to {@link Method#FILE_LOADS}, and only when writing an unbounded {@link PCollection}.However, we use triggering frequency for STORAGE_WRITE_API and STORAGE_API_AT_LEAST_ONCE as well. This documentation should be updated to reflect this.In addition, during the validation component of BigQueryIO.write().expand(), we check for unbounded && (file loads || storage write api) to see if a triggering frequency is set. This should probably also be checked if storage write at least once is set.
-1	https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/.There are a variety of tests failing, and each run has a different failure.However, after looking through it seems that test_pardo_large_input is one of the more commonly failing tests.Here's a link to the logs for one of the failures test_pardo_large_input
1	#23908 adds some basic examples of Batched DoFns to the Beam Programming Guide, but it would be good to also add some more end-to-end examples, in example pipelines and/or in notebooks.
-1	I have deployed a Spark v3.1.2 cluster on kubernetes. My beam job server and beam sdk container are running on 2 separate linux virtual machines. The following keeps executing and does not stop op = PipelineOptions([ "--runner=PortableRunner",         "--job_endpoint=localhost:8099", "--environment_type=EXTERNAL", "--environment_config=vm2-hostname::50000", "--artifact_endpoint=localhost:8098"   ] ).with beam.Pipeline(options=op) as p: p | beam.Create([1, 2, 3, 10]) | beam.Map(lambda x: x+1) | beam.Map(print).The docker logs for the sdk container show the following error caused by:context deadline exceeded
1	Simultaneously run examples in different SDKs
-1	HTML renderer saves ~2MB of download size compared to CanvasKit renderer.The problem is some SVG icons appear corrupted.There may be other issues as well.
-1	Jenkins test has been failing since October
-1	optimizeOuterThis, an optimization in JDK 11, causes a serialization failure with org.apache.beam.sdk.transforms.Count$CountFn$1. We can disable this optimization for now, but serializing anonymous classes is strongly discouraged. We should consider refactoring.
-1	Hi, @robertwb.Issue Description.Package org.apache.beam.runners.core is a promiscuous package, and groups together miscellaneous functionalities that might be useful to different subsystems. The package structure violates the “high cohesion and low coupling” design rules. I found that Class InMemoryBundleFinalizer is not used by classes in their package. The project has grown larger, leading to becoming increasingly hard to maintain. During the development process, one groups together classes (that often co-change) with similar responsibilities in one package to facilitate maintenance, which prevents a change that causes other packages to be modified. For example, if one modifies package org.apache.beam.runners.core (i.e., package rename), resulting in multiple classes of package org.apache.beam.runners.direct to be changed.Location: The source file can be found at path File  runners/core-java/src/main/java/org/apache/beam/runners/core.Refactoring suggestions.I suggest to move class InMemoryBundleFinalizer ito package org.apache.beam.runners.direct.
1	Hi Team,We have been planning to use dataflow with spanner, while doing POC i noticed the SpannerId.read was trying to read complete table.Resulting more time in processing, can we make it like it keeps dropping batch of data already reading like we do in window?
1	Updating the endpoints of precompiled objects to get datasets
-1	Setup monitoring and email alerts for ToB endpoints which cover:bad http codes/internal service errors: panics, cold restarts.Consider implementing as terraform script
1	Benchmark RunInference frameworks on GPUs and CPUs. Provide information on the website on the suggestions on how to use GPUs.
-1	The Akvelon Editor dependency is showing a 10/30 static code analysis score.  See https://pub.dev/packages/flutter_code_editor/score.The flutter_code_editor is a dependency in the Beam Playground.
1	Update the documentation to explain the use of python external transforms in Go pipelines similar to Java
-1	There may be models which are not thread safe and will run into issues using RunInference (which relies on shared.py under the covers). We should add the ability to force type safety via locking when performing inference.
1	Investigate performance issue of Spark dataset runner (SparkStructuredStreamingRunner) for TPC-DS query 83.This might be related to handling of side inputs, but not confirmed.
-1	Thanks. Python Precommit showing following test failure: https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/6286/.TypeError: __init__() got an unexpected keyword argument 'vpt_vp_arg3'.Originally posted by @Abacn in #22421 (comment)
-1	What would you like to happen?When a batch of inferences fails, we would ideally like to be informed which record caused the failure so that it can be investigated directly.
1	Emit a WriteResult from DynamoDBIO.write, which would make it possible to take other actions afterwards.
-1	Currently we refuse to map Beam logical types to arrow. It should be possible to put the necessary metadata into arrow's custom metadata field (as we do for Beam options, and the schema id).
-1	Beam schemas do not support unsigned ints or float16s, so we currently refuse to map those in arrow_type_compatibility. If these types can't be added to core Beam schemas we might consider adding them as logical types.
1	Bump Spark3 version to 3.2.0 as a main Spark version in Beam
1	The Spark related roadmap is far outdated and needs some attention.Spark 3 is fully supported.Spark dataset runner is on it's way.
1	Currently examples use conext-line hidden comment with an index of a line to scroll to.Allow to mark a line with // scroll-to comment instead.
1	Hi, Beam community,I very much like the feature WriteToBigquery offering to dynamically deciding the table:I am thinking about a potential improvement opportunity. Currently WriteToBigquery supports either dict or TableRow, and it assumes that table reference can be inferred from the content of dict or TableRow to be written into BQ. We probably should remove that assumption. For systems that does not align with this assumption the operator would force them to add table name as a column. (or optionally enable ignore_unknown_fields).This is a case I am expecting WriteToBigquery to support:Given a PCollection of K, V pair of <str, dict>, I hope K can be the used to infer table name yet get V written into the dictionary. (or even do some further processing on V).I can see there is a few way to do that. The simplest way I can think of is to allow table arg to be an instance of DoFn or AppendDestinationsFn.
1	if the sdk allow us to overwrite AppendDestinationsFn and pass it into WriteToBigquery we will have more freedom to customise the class.I am happy to contribute and make this feature come true. But would be keen to get communities' feedback first:)
-1	On an unrelated PR, this failure occurred: https://ci-beam.apache.org/job/beam_PreCommit_Java_Commit/24482/testReport/junit/org.apache.beam.runners.samza.runtime/AsyncDoFnRunnerTest/testSimplePipeline/
-1	Currently, the GCP Vision API implemented in python SDK doesn't return the image_uri which makes it difficult to know which image does the response correspond to.
-1	I'm trying to use apache beam go sdk to listen to messages from kafka. This is how I'm trying to use kafkaio library to connect to kafka.I followed the instructions given in https://github.com/apache/beam/blob/master/sdks/go/examples/kafka/taxi.go example to write this. I have kafka setup and running locally. As instructed, I also have the expansion service running too. But, when I try to run this, I'm getting no root units found error with the following logs
-1	Upon debugging, I found the logic where we're determining the root expects an edge with OpType Impulse and since the above pipeline graph doesn't contain any root nodes, the job failed. I was able to resolve this error by just adding a dummy textio.Read to the pipeline (commented line in the code shared above). But, then the kafka consumer was not listening to the messages. The program ended without any kafka listener listening to the topic. I'm guessing this is happening since it won't be part of the execution plan since there was no root detected in that part of the pipeline. Am I missing anything? Are there any more examples of go sdk using kafka connect?Note: I was able to connect and listen to kafka messages using the java client. So, I suspect it has some issues related to cross language communication based on which the kafkaio is implemented.
-1	Currently log level and log level overrides are handled by fn harness: For PipelineRunners that run pipeline using translator, e.g., FlinkRunner, SparkRunner, but not involve fn harness, these options does not take effect. (Flink support is in #23635). We need support  default log level and log level overrides on these runners.
-1	This test has been historically flaky, and flaked here:https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/10934/ (https://scans.gradle.com/s/kqx3lxltwtorq#failure)
1	The gradle task definition for {runner}PreCommit was copy-pasted from the java example without adjusting test includes.Currently no tests are run
-1	Unfortunately InfluxQL is very limited. Many, rather basic types of queries can only be done using the Flux query interface. E.g. extracting parts from a string for readability or also grouping by a dynamic set of columns as required to handle Java microbenchmark (JMH) params.
-1	Grafana supports Flux running on InfluxDB 1.8+. See 1.8 compatibility for more information and connection details.For 1.8 it has to be enabled using INFLUXDB_HTTP_FLUX_ENABLED=true.Though, it looks like the versions are incompatible. I wasn't able to successfully test it without upgrading InfluxDB to 2.x. But it might have been misconfiguration.panic: column retentionPeriod:int is not of type string
-1	This is how font sizes currently compare on my 1920x1080 screen with the default Chrome settings:We should:Make the sizes match the Beam website.Ensure font size scales following browser settings.
